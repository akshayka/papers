<!DOCTYPE html>
<html>
<head>
  <link href="https://fonts.googleapis.com/css?family=PT+Serif" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
  <link href='../style.css' rel='stylesheet'>
  <meta name=viewport content="width=device-width, initial-scale=1">
</head>

<body>
  <div id="container">
<h1 id="a-latent-variable-model-approach-to-pmi-based-word-embeddings-arora-2016httpstransaclorgojsindexphptaclarticleviewfile742204"><a href="https://transacl.org/ojs/index.php/tacl/article/viewFile/742/204">A Latent Variable Model Approach to PMI-based Word Embeddings (Arora 2016)</a></h1>

<h2 id="background">Background</h2>
<p>There are two popular classes of word embedding techniques:</p>

<ol>
  <li>Compute a low-rank approximation of a re-weighted co-occurrence matrix
(i.e., PCA via SVD).</li>
  <li>Use a neural network language model’s representation of a word (e.g.,
word2vec, GloVe).</li>
</ol>

<p>A re-weighting scheme used in (1) is to replace the co-occurrence statistics
with the <em>pointwise mutual information</em> between two words.</p>

<p hidden="">
$$
\newcommand{\pmi}{\operatorname{pmi}}
\newcommand{\inner}[2]{\langle{#1}, {#2}\rangle}
\newcommand{\Pb}{\operatorname{Pr}}
$$
</p>

<p>The pointwise mutual information (PMI) of a pair of outcomes <script type="math/tex">x \in X</script>,
<script type="math/tex">y \in Y</script>, <script type="math/tex">X</script>, <script type="math/tex">Y</script> discrete random variables measures the extent to
which their joint distribution differs from the product of the marginal
distributions:</p>

<script type="math/tex; mode=display">\pmi(x;y) := \log\frac{p(x,y)}{p(x)p(y)} = \log \frac{p(x \mid y)}{p(x)}
= \log \frac{p(y \mid x)}{p(y)}</script>

<p>Note that <script type="math/tex">\pmi(x;y)</script> attains its maximum when <script type="math/tex">p(x \mid y) = 1</script> or <script type="math/tex">p(y \mid x) = 1</script>.</p>

<p>It is observed empirically that</p>

<script type="math/tex; mode=display">\begin{align}
\inner{v_w}{v_{w'}} \approx \pmi(w, w') \label{low-rank-pmi}.
\end{align}</script>

<h2 id="key-contribution">Key Contribution</h2>
<p><strong>This paper proposes a generative model for word embeddings that provides a
theoretical justification of</strong> <script type="math/tex">(\ref{low-rank-pmi})</script> <strong>and word2vec
and GloVe</strong>. The key assumption it makes is that word vectors, the latent
variables of the model, are spatially isotropic (intuition: “no preferred
direction in space”. Isotropy of low-dimensional vectors helps explain
the linear structure of word vectors as well.</p>

<h2 id="the-generative-model">The Generative Model</h2>
<p>A time-step model: at time <script type="math/tex">t</script>, word <script type="math/tex">t</script> is produced by a random walk of
a discourse vector <script type="math/tex">c_t \in \mathbb{R}^d</script> that represents the topic of
conversation. Each generated word has a latent vector <script type="math/tex">v_w \in \mathbb{R}^d</script>
that measures the correlation with the discourse vector. In particular:</p>

<p><script type="math/tex">\Pb(w</script> is the  <script type="math/tex">t</script>-th word <script type="math/tex">\mid c_t) \propto \exp \inner{c_t}{v_w}</script>.</p>

<p><script type="math/tex">c_{t+1} := c_t +</script> a small random displacement. <strong>Under this model, the authors
prove that the co-occurrence probabilities and marginal probabilities
are functions of the word vectors; optimizing this relation is one way of
materializing the actual word vectors.</strong></p>

<h2 id="commentary">Commentary</h2>
<ul>
  <li>This paper answers an interesting question: Why is it that a nonlinear model
like word2vec produces outputs that have linear structures
(e.g., king - man = woman)?</li>
</ul>

  <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        "HTML-CSS": { availableFonts: ["TeX"] },
         TeX: { equationNumbers: { autoNumber: "AMS" } },
      });
   </script>
   <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
  </div>
</body>
</html>
