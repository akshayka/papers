<!DOCTYPE html>
<html>
<head>
  <link href="https://fonts.googleapis.com/css?family=PT+Serif" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
  <link href='../style.css' rel='stylesheet'>
  <meta name=viewport content="width=device-width, initial-scale=1">
  <meta http-equiv='Content-Type' content='text/html; charset=utf-8' />
</head>

<body>
  <div id="container">
    <center>
      <h1 id="indextitle"><a class="title" href="../index.html">Papers</a></h1>
    </center>

  <p hidden>
  $$
  \newcommand{\pmi}{\operatorname{pmi}}
  \newcommand{\inner}[2]{\langle{#1}, {#2}\rangle}
  \newcommand{\Pb}{\operatorname{Pr}}
  \newcommand{\E}{\mathbb{E}}
  \newcommand{\RR}{\mathbf{R}}
  \newcommand{\script}[1]{\mathcal{#1}}
  \newcommand{\Set}[2]{\{{#1} : {#2}\}}
  \newcommand{\argmin}[2]{\underset{#1}{\operatorname{argmin}} {#2}}
  \newcommand{\optmin}[3]{
	\begin{align*}
	& \underset{#1}{\text{minimize}} & & #2 \\
	& \text{subject to} & & #3
	\end{align*}
  }
  \newcommand{\optmax}[3]{
	\begin{align*}
	& \underset{#1}{\text{maximize}} & & #2 \\
	& \text{subject to} & & #3
	\end{align*}
  }
  \newcommand{\optfind}[2]{
	\begin{align*}
	& {\text{find}} & & #1 \\
	& \text{subject to} & & #2
	\end{align*}
  }
  $$
  </p>

<h1 id="a-latent-variable-model-approach-to-pmi-based-word-embeddings-arora-2016httpstransaclorgojsindexphptaclarticleviewfile742204"><a href="https://transacl.org/ojs/index.php/tacl/article/viewFile/742/204">A Latent Variable Model Approach to PMI-based Word Embeddings (Arora 2016)</a></h1>

<h2 id="background">Background</h2>
<p>There are two popular classes of word embedding techniques:</p>

<ol>
  <li>Compute a low-rank approximation of a re-weighted co-occurrence matrix
(i.e., PCA via SVD).</li>
  <li>Use a neural network language model’s representation of a word (e.g.,
word2vec, GloVe).</li>
</ol>

<p>A re-weighting scheme used in (1) is to replace the co-occurrence statistics
with the <em>pointwise mutual information</em> between two words.</p>

<p>The pointwise mutual information (PMI) of a pair of outcomes <script type="math/tex">x \in X</script>,
<script type="math/tex">y \in Y</script>, <script type="math/tex">X</script>, <script type="math/tex">Y</script> discrete random variables measures the extent to
which their joint distribution differs from the product of the marginal
distributions:</p>

<script type="math/tex; mode=display">\pmi(x;y) := \log\frac{p(x,y)}{p(x)p(y)} = \log \frac{p(x \mid y)}{p(x)}
= \log \frac{p(y \mid x)}{p(y)}</script>

<p>Note that <script type="math/tex">\pmi(x;y)</script> attains its maximum when <script type="math/tex">p(x \mid y) = 1</script> or <script type="math/tex">p(y \mid x) = 1</script>.</p>

<p>It is observed empirically that</p>

<script type="math/tex; mode=display">\begin{align}
\inner{v_w}{v_{w'}} \approx \pmi(w, w') \label{low-rank-pmi}.
\end{align}</script>

<h2 id="key-contribution">Key Contribution</h2>
<p><em>This paper proposes a generative model for word embeddings that provides a
theoretical justification of</em> <script type="math/tex">(\ref{low-rank-pmi})</script> <em>and word2vec
and GloVe</em>. The key assumption it makes is that word vectors, the latent
variables of the model, are spatially isotropic (intuition: “no preferred
direction in space”). Isotropy of low-dimensional vectors helps explain
the linear structure of word vectors as well.</p>

<h2 id="the-generative-model">The Generative Model</h2>
<p>A time-step model: at time <script type="math/tex">t</script>, word <script type="math/tex">t</script> is produced by a random walk of
a discourse vector <script type="math/tex">c_t \in \mathbb{R}^d</script> that represents the topic of
conversation. Each generated word has a latent vector <script type="math/tex">v_w \in \mathbb{R}^d</script>
that measures the correlation with the discourse vector. In particular:</p>

<p><script type="math/tex">\Pb(w</script> is the  <script type="math/tex">t</script>-th word <script type="math/tex">\mid c_t) \propto \exp \inner{c_t}{v_w}</script>.</p>

<p><script type="math/tex">c_{t+1} := c_t +</script> a small random displacement. <em>Under this model, the authors
prove that the co-occurrence probabilities and marginal probabilities
are functions of the word vectors; this is useful when optimizing the
likelihood function</em> <script type="math/tex">\ell = \log \prod_{w, w'} p(w, w')^{X_{w,w'}}</script>.</p>

<h2 id="commentary">Commentary</h2>
<ul>
  <li>This paper answers an interesting question: Why is it that a nonlinear model
like word2vec produces outputs that have linear structures
(e.g., king - man = woman)?</li>
  <li>It’s really cool that a relatively simple generative model grounded in a
solid theoretical foundation produces results that are competitive with
neural network models.</li>
</ul>

  <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        "HTML-CSS": { availableFonts: ["TeX"] },
         TeX: { equationNumbers: { autoNumber: "AMS" } },
      });
   </script>
   <script type="text/javascript" async
       src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
  </div>
</body>
</html>
