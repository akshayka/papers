<!DOCTYPE html>
<html>
<head>
  <link href="https://fonts.googleapis.com/css?family=PT+Serif" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
  <link href='../style.css' rel='stylesheet'>
  <meta name=viewport content="width=device-width, initial-scale=1">
  <meta http-equiv='Content-Type' content='text/html; charset=utf-8' />
</head>

<body>
  <div id="container">
    <center>
      <h1 id="indextitle"><a class="title" href="../index.html">Papers</a></h1>
    </center>

  <p hidden>
  $$
  \newcommand{\pmi}{\operatorname{pmi}}
  \newcommand{\inner}[2]{\langle{#1}, {#2}\rangle}
  \newcommand{\Pb}{\operatorname{Pr}}
  \newcommand{\E}{\mathbb{E}}
  \newcommand{\argmin}[2]{\underset{#1}{\operatorname{argmin}} {#2}}
  \newcommand{\optmin}[3]{
	\begin{align*}
	& \underset{#1}{\text{minimize}} & & #2 \\
	& \text{subject to} & & #3
	\end{align*}
  }
  \newcommand{\optmax}[3]{
	\begin{align*}
	& \underset{#1}{\text{maximize}} & & #2 \\
	& \text{subject to} & & #3
	\end{align*}
  }
  \newcommand{\optfind}[2]{
	\begin{align*}
	& {\text{find}} & & #1 \\
	& \text{subject to} & & #2
	\end{align*}
  }
  $$
  </p>

<h1 id="train-faster-generalize-better-stability-of-stochastic-gradient-descent-hardt-2016httpsarxivorgpdf150901240v2pdf"><a href="https://arxiv.org/pdf/1509.01240v2.pdf">Train Faster, Generalize Better: Stability of Stochastic Gradient Descent (Hardt 2016)</a></h1>

<p>Nut graf: fast training prevents overfitting, and a mathematical way to state
          this.</p>

<h2 id="background">Background</h2>
<p>Recall a <script type="math/tex">\textbf{defintion}</script>: Let <script type="math/tex">\mathcal{D}</script> be the training data,
and assume both <script type="math/tex">\mathcal{D}</script> and the test data are generated by draws
from the same distribution <script type="math/tex">X</script>. Let
<script type="math/tex">e(w; x)</script> be the error incurred by a model parametrized by <script type="math/tex">s</script> on a set
of samples <script type="math/tex">x</script>, where each sample is drawn from <script type="math/tex">X</script>.
The <script type="math/tex">\textbf{generalization error}</script> is defined as</p>

<script type="math/tex; mode=display">\E[e(w; x)]- e(w; \mathcal{D}), \quad x \sim X, \quad x \notin \mathcal{D}.</script>

<p>The former term is the population risk and the latter term is the empirical
risk. If the weights <script type="math/tex">w=A(S)</script>, <script type="math/tex">S</script> a sample, <script type="math/tex">A</script> a randomized function,
then the expected generalization error is defined as the expectation
of the generalization error, taken over the randomness in <script type="math/tex">S</script> and <script type="math/tex">A</script>.
And if two samples <script type="math/tex">S</script> and <script type="math/tex">S'</script> differ in at most one example,
then <script type="math/tex">A</script> is <script type="math/tex">\epsilon</script>-<script type="math/tex">\textbf{uniformly stable}</script> if</p>

<script type="math/tex; mode=display">\sup_{z}\E_{A}[f(A(S); z) - f(A(S'); z)] \leq \epsilon,</script>

<p>where <script type="math/tex">f</script> the loss function evaluated at <script type="math/tex">z</script>.
<em>Key idea: such an algorithm has generalization error bounded by <script type="math/tex">\epsilon</script>.</em></p>

<h2 id="results">Results</h2>
<p>The key results are stability bounds for the application of SGD to
<script type="math/tex">\beta</script>-smooth, <script type="math/tex">L</script>-Lipschitz, convex functions and certain classes
of nonconvex functions. The results for the former require each step size
to be bounded by <script type="math/tex">\approx 1/\beta</script>. The results for the latter require
that the function lie in <script type="math/tex">[0, 1]</script>, is L-Lipschitz, and <script type="math/tex">\beta</script>-smooth,
and that the sequence of step sizes is inversely related to the step number.</p>

<p>The upshots are cool: regularization, gradient clipping, dropout, proximal
steps, and model averaging all improve the authors’ bounds.</p>

<h2 id="commentary">Commentary</h2>
<ul>
  <li>Are machine learning loss functions often Lipschitz and smooth? Is this too
stringent of an assumption?</li>
  <li>Again, are the assumptions made for the nonconvex case reasonable?</li>
  <li>It’s cool that the bounds match empirical findings.</li>
</ul>
  <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        "HTML-CSS": { availableFonts: ["TeX"] },
         TeX: { equationNumbers: { autoNumber: "AMS" } },
      });
   </script>
   <script type="text/javascript" async
       src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
  </div>
</body>
</html>
